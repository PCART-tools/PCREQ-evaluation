SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  228.4246, NLL-Loss  228.4239, KL-Loss    0.3800, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  149.8329, NLL-Loss  149.7759, KL-Loss   26.1179, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  143.5442, NLL-Loss  143.4774, KL-Loss   27.0080, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  139.9500, NLL-Loss  139.8403, KL-Loss   39.1768, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  127.4725, NLL-Loss  127.3213, KL-Loss   47.6544, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  149.2703, NLL-Loss  149.0644, KL-Loss   57.2954, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  120.2400, NLL-Loss  119.9950, KL-Loss   60.1852, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  146.0202, NLL-Loss  145.7481, KL-Loss   59.0312, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  129.1596, NLL-Loss  128.8201, KL-Loss   65.0385, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  118.8843, NLL-Loss  118.5161, KL-Loss   62.3049, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  120.3087, NLL-Loss  119.8652, KL-Loss   66.2628, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  109.2918, NLL-Loss  108.7922, KL-Loss   65.9231, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  126.4181, NLL-Loss  125.8431, KL-Loss   67.0400, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  116.8073, NLL-Loss  116.2165, KL-Loss   60.8533, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss   97.8983, NLL-Loss   97.2346, KL-Loss   60.4153, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  109.2514, NLL-Loss  108.5094, KL-Loss   59.6840, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  118.8435, NLL-Loss  117.9915, KL-Loss   60.5828, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  115.7586, NLL-Loss  114.8240, KL-Loss   58.7529, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss  106.1911, NLL-Loss  105.1525, KL-Loss   57.7431, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss  113.3534, NLL-Loss  112.2142, KL-Loss   56.0310, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss  112.0312, NLL-Loss  110.7910, KL-Loss   53.9720, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  126.8062, NLL-Loss  125.4581, KL-Loss   51.9359, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  103.4664, NLL-Loss  101.9930, KL-Loss   50.2678, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  103.4174, NLL-Loss  101.8308, KL-Loss   47.9548, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss  114.8326, NLL-Loss  113.0347, KL-Loss   48.1680, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  113.3691, NLL-Loss  111.3530, KL-Loss   47.9016, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss   99.5460, NLL-Loss   97.3419, KL-Loss   46.4744, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss  120.9988, NLL-Loss  118.7418, KL-Loss   46.0313, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.9511
Model saved at bin/2025-Dec-13-05:16:53/E0.pytorch
VALID Batch 0000/105, Loss  124.5607, NLL-Loss  122.4687, KL-Loss   42.5650, KL-Weight  0.049
VALID Batch 0050/105, Loss  124.9836, NLL-Loss  122.7363, KL-Loss   45.7230, KL-Weight  0.049
VALID Batch 0100/105, Loss   89.0018, NLL-Loss   87.0558, KL-Loss   39.5942, KL-Weight  0.049
VALID Batch 0105/105, Loss   91.8619, NLL-Loss   89.6211, KL-Loss   45.5915, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  104.8573
