SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  211.5142, NLL-Loss  211.5135, KL-Loss    0.3739, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  174.9494, NLL-Loss  174.9124, KL-Loss   16.9521, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  146.4370, NLL-Loss  146.3664, KL-Loss   28.5381, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  149.1916, NLL-Loss  149.0657, KL-Loss   44.9491, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  129.0899, NLL-Loss  128.9384, KL-Loss   47.7431, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  139.6101, NLL-Loss  139.4295, KL-Loss   50.2542, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  138.6355, NLL-Loss  138.4214, KL-Loss   52.6133, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  108.5679, NLL-Loss  108.2917, KL-Loss   59.9210, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  109.4563, NLL-Loss  109.1242, KL-Loss   63.6131, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  127.0180, NLL-Loss  126.6418, KL-Loss   63.6435, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  103.7439, NLL-Loss  103.3067, KL-Loss   65.3302, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  135.3498, NLL-Loss  134.8613, KL-Loss   64.4696, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  129.2035, NLL-Loss  128.6919, KL-Loss   59.6448, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  102.6060, NLL-Loss  102.0289, KL-Loss   59.4422, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss  119.2659, NLL-Loss  118.5724, KL-Loss   63.1136, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  126.3968, NLL-Loss  125.6539, KL-Loss   59.7599, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  106.6383, NLL-Loss  105.8241, KL-Loss   57.8935, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  123.8293, NLL-Loss  122.9025, KL-Loss   58.2711, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss  120.3256, NLL-Loss  119.3888, KL-Loss   52.0862, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss  108.7822, NLL-Loss  107.6512, KL-Loss   55.6276, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss  118.9658, NLL-Loss  117.7378, KL-Loss   53.4455, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  117.2412, NLL-Loss  115.8753, KL-Loss   52.6223, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  133.8305, NLL-Loss  132.2760, KL-Loss   53.0320, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  102.8099, NLL-Loss  101.2399, KL-Loss   47.4515, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss  100.0919, NLL-Loss   98.3771, KL-Loss   45.9413, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  120.7186, NLL-Loss  118.7075, KL-Loss   47.7827, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss   95.5600, NLL-Loss   93.4068, KL-Loss   45.4001, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss   99.3470, NLL-Loss   97.3334, KL-Loss   41.0673, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  120.0027
Model saved at bin/2025-Dec-21-07:40:14/E0.pytorch
VALID Batch 0000/105, Loss  124.7990, NLL-Loss  122.7312, KL-Loss   42.0719, KL-Weight  0.049
VALID Batch 0050/105, Loss  123.5730, NLL-Loss  121.3483, KL-Loss   45.2651, KL-Weight  0.049
VALID Batch 0100/105, Loss   90.5905, NLL-Loss   88.7251, KL-Loss   37.9551, KL-Weight  0.049
VALID Batch 0105/105, Loss   90.3444, NLL-Loss   88.0969, KL-Loss   45.7274, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  104.9336
