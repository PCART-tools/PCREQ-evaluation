SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  214.8136, NLL-Loss  214.8128, KL-Loss    0.4086, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  164.1387, NLL-Loss  164.0882, KL-Loss   23.1427, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  130.6472, NLL-Loss  130.5813, KL-Loss   26.6463, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  135.5975, NLL-Loss  135.5020, KL-Loss   34.0800, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  117.2182, NLL-Loss  117.0632, KL-Loss   48.8492, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  131.3278, NLL-Loss  131.1310, KL-Loss   54.7635, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  127.4604, NLL-Loss  127.2141, KL-Loss   60.5370, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  108.8862, NLL-Loss  108.6039, KL-Loss   61.2430, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  130.4464, NLL-Loss  130.0997, KL-Loss   66.4074, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  132.0963, NLL-Loss  131.7324, KL-Loss   61.5588, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  106.8558, NLL-Loss  106.4225, KL-Loss   64.7318, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  122.2751, NLL-Loss  121.7642, KL-Loss   67.4258, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss   98.2165, NLL-Loss   97.6717, KL-Loss   63.5242, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss   97.8135, NLL-Loss   97.1753, KL-Loss   65.7383, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss  117.7057, NLL-Loss  117.0160, KL-Loss   62.7793, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  134.9633, NLL-Loss  134.1653, KL-Loss   64.1906, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  119.0745, NLL-Loss  118.1656, KL-Loss   64.6277, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  115.0814, NLL-Loss  114.1542, KL-Loss   58.2907, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss  106.9623, NLL-Loss  105.9094, KL-Loss   58.5428, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss   93.6240, NLL-Loss   92.4482, KL-Loss   57.8298, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss   84.7719, NLL-Loss   83.5368, KL-Loss   53.7513, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  125.0534, NLL-Loss  123.6217, KL-Loss   55.1582, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  129.7886, NLL-Loss  128.2817, KL-Loss   51.4085, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss   97.9184, NLL-Loss   96.2336, KL-Loss   50.9221, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss  106.5795, NLL-Loss  104.7321, KL-Loss   49.4937, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  106.3334, NLL-Loss  104.4411, KL-Loss   44.9620, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss  112.6884, NLL-Loss  110.4406, KL-Loss   47.3972, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss  112.8750, NLL-Loss  110.7168, KL-Loss   44.0154, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.9397
Model saved at bin/2025-Dec-13-06:13:04/E0.pytorch
VALID Batch 0000/105, Loss  124.5616, NLL-Loss  122.4352, KL-Loss   43.2658, KL-Weight  0.049
VALID Batch 0050/105, Loss  124.4562, NLL-Loss  122.1558, KL-Loss   46.8030, KL-Weight  0.049
VALID Batch 0100/105, Loss   91.7588, NLL-Loss   89.8164, KL-Loss   39.5201, KL-Weight  0.049
VALID Batch 0105/105, Loss   93.1944, NLL-Loss   90.9968, KL-Loss   44.7138, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  104.9823
