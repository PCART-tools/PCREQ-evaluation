SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  203.8376, NLL-Loss  203.8368, KL-Loss    0.3806, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  109.4160, NLL-Loss  109.3756, KL-Loss   18.5122, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  128.2621, NLL-Loss  128.1895, KL-Loss   29.3814, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  157.6621, NLL-Loss  157.5542, KL-Loss   38.5466, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  120.5231, NLL-Loss  120.3725, KL-Loss   47.4732, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  121.2920, NLL-Loss  121.0882, KL-Loss   56.6998, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  120.2423, NLL-Loss  120.0073, KL-Loss   57.7316, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  114.3185, NLL-Loss  114.0649, KL-Loss   55.0120, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  132.3267, NLL-Loss  132.0012, KL-Loss   62.3585, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  137.1867, NLL-Loss  136.8263, KL-Loss   60.9652, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  104.2641, NLL-Loss  103.8638, KL-Loss   59.8111, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  132.3558, NLL-Loss  131.8934, KL-Loss   61.0175, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  102.5956, NLL-Loss  102.0614, KL-Loss   62.2820, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  118.1219, NLL-Loss  117.4827, KL-Loss   65.8419, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss  118.8925, NLL-Loss  118.2194, KL-Loss   61.2685, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss   94.2939, NLL-Loss   93.5295, KL-Loss   61.4907, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  108.6323, NLL-Loss  107.7968, KL-Loss   59.4056, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  113.3536, NLL-Loss  112.4044, KL-Loss   59.6726, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss  104.1882, NLL-Loss  103.2133, KL-Loss   54.2033, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss  109.9989, NLL-Loss  108.8683, KL-Loss   55.6093, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss   94.2346, NLL-Loss   93.0493, KL-Loss   51.5859, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  105.1029, NLL-Loss  103.7318, KL-Loss   52.8234, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  104.0494, NLL-Loss  102.5466, KL-Loss   51.2681, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  111.7559, NLL-Loss  110.1161, KL-Loss   49.5628, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss   94.8515, NLL-Loss   93.1057, KL-Loss   46.7690, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  121.0142, NLL-Loss  119.0012, KL-Loss   47.8291, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss  105.8618, NLL-Loss  103.7753, KL-Loss   43.9957, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss  114.7701, NLL-Loss  112.7391, KL-Loss   41.4214, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.7869
Model saved at bin/2025-Dec-14-18:30:18/E0.pytorch
VALID Batch 0000/105, Loss  126.3087, NLL-Loss  124.2817, KL-Loss   41.2412, KL-Weight  0.049
VALID Batch 0050/105, Loss  123.7967, NLL-Loss  121.4836, KL-Loss   47.0645, KL-Weight  0.049
VALID Batch 0100/105, Loss   89.8181, NLL-Loss   87.9454, KL-Loss   38.1026, KL-Weight  0.049
VALID Batch 0105/105, Loss   91.0245, NLL-Loss   88.8051, KL-Loss   45.1572, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  105.0327
