SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  205.1219, NLL-Loss  205.1212, KL-Loss    0.3527, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  150.7057, NLL-Loss  150.6638, KL-Loss   19.2295, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  149.0208, NLL-Loss  148.9473, KL-Loss   29.6989, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  145.8640, NLL-Loss  145.7518, KL-Loss   40.0352, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  143.1705, NLL-Loss  143.0105, KL-Loss   50.4562, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  124.8571, NLL-Loss  124.6709, KL-Loss   51.8066, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  135.4879, NLL-Loss  135.2466, KL-Loss   59.2734, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  120.1292, NLL-Loss  119.8215, KL-Loss   66.7654, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  125.2425, NLL-Loss  124.8884, KL-Loss   67.8253, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  122.4242, NLL-Loss  122.0261, KL-Loss   67.3463, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  127.1063, NLL-Loss  126.6708, KL-Loss   65.0605, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  110.8860, NLL-Loss  110.3896, KL-Loss   65.5142, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  122.7049, NLL-Loss  122.1595, KL-Loss   63.5951, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  111.7591, NLL-Loss  111.1359, KL-Loss   64.1885, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss   98.0682, NLL-Loss   97.3681, KL-Loss   63.7147, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  130.5096, NLL-Loss  129.7513, KL-Loss   60.9985, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  109.6658, NLL-Loss  108.8067, KL-Loss   61.0850, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  106.8052, NLL-Loss  105.8208, KL-Loss   61.8855, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss  103.4944, NLL-Loss  102.5172, KL-Loss   54.3290, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss  104.4115, NLL-Loss  103.2062, KL-Loss   59.2832, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss  110.7634, NLL-Loss  109.4790, KL-Loss   55.8993, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  118.0925, NLL-Loss  116.5543, KL-Loss   59.2617, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  102.0742, NLL-Loss  100.4979, KL-Loss   53.7770, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  118.8366, NLL-Loss  117.1168, KL-Loss   51.9798, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss   79.2953, NLL-Loss   77.5343, KL-Loss   47.1768, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss   99.7062, NLL-Loss   97.7557, KL-Loss   46.3428, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss  121.6983, NLL-Loss  119.3879, KL-Loss   48.7163, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss  114.0564, NLL-Loss  111.9110, KL-Loss   43.7546, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.7571
Model saved at bin/2025-Dec-14-16:12:01/E0.pytorch
VALID Batch 0000/105, Loss  124.8677, NLL-Loss  122.7531, KL-Loss   43.0246, KL-Weight  0.049
VALID Batch 0050/105, Loss  124.6983, NLL-Loss  122.4177, KL-Loss   46.4031, KL-Weight  0.049
VALID Batch 0100/105, Loss   89.5949, NLL-Loss   87.5726, KL-Loss   41.1454, KL-Weight  0.049
VALID Batch 0105/105, Loss   91.6869, NLL-Loss   89.4958, KL-Loss   44.5815, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  104.5669
