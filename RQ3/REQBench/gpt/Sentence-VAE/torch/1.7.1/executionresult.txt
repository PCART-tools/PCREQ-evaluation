SentenceVAE(
  (embedding): Embedding(9877, 300)
  (embedding_dropout): Dropout(p=0.5, inplace=False)
  (encoder_rnn): GRU(300, 256, batch_first=True)
  (decoder_rnn): GRU(300, 256, batch_first=True)
  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)
  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)
  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)
  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)
)
TRAIN Batch 0000/1314, Loss  198.6574, NLL-Loss  198.6567, KL-Loss    0.3538, KL-Weight  0.002
TRAIN Batch 0050/1314, Loss  151.2073, NLL-Loss  151.1643, KL-Loss   19.6905, KL-Weight  0.002
TRAIN Batch 0100/1314, Loss  146.3933, NLL-Loss  146.3324, KL-Loss   24.6358, KL-Weight  0.002
TRAIN Batch 0150/1314, Loss  138.6882, NLL-Loss  138.5587, KL-Loss   46.2117, KL-Weight  0.003
TRAIN Batch 0200/1314, Loss  133.7730, NLL-Loss  133.6003, KL-Loss   54.4562, KL-Weight  0.003
TRAIN Batch 0250/1314, Loss  110.3504, NLL-Loss  110.1347, KL-Loss   60.0118, KL-Weight  0.004
TRAIN Batch 0300/1314, Loss  139.5515, NLL-Loss  139.2863, KL-Loss   65.1689, KL-Weight  0.004
TRAIN Batch 0350/1314, Loss  119.0717, NLL-Loss  118.7745, KL-Loss   64.4819, KL-Weight  0.005
TRAIN Batch 0400/1314, Loss  120.6513, NLL-Loss  120.2996, KL-Loss   67.3850, KL-Weight  0.005
TRAIN Batch 0450/1314, Loss  105.3861, NLL-Loss  105.0182, KL-Loss   62.2383, KL-Weight  0.006
TRAIN Batch 0500/1314, Loss  131.2022, NLL-Loss  130.7484, KL-Loss   67.8162, KL-Weight  0.007
TRAIN Batch 0550/1314, Loss  115.1117, NLL-Loss  114.6077, KL-Loss   66.5044, KL-Weight  0.008
TRAIN Batch 0600/1314, Loss  108.0476, NLL-Loss  107.4780, KL-Loss   66.4066, KL-Weight  0.009
TRAIN Batch 0650/1314, Loss  121.7762, NLL-Loss  121.1782, KL-Loss   61.5969, KL-Weight  0.010
TRAIN Batch 0700/1314, Loss  120.4719, NLL-Loss  119.7734, KL-Loss   63.5787, KL-Weight  0.011
TRAIN Batch 0750/1314, Loss  110.1178, NLL-Loss  109.3533, KL-Loss   61.4954, KL-Weight  0.012
TRAIN Batch 0800/1314, Loss  108.7106, NLL-Loss  107.8403, KL-Loss   61.8827, KL-Weight  0.014
TRAIN Batch 0850/1314, Loss  105.5362, NLL-Loss  104.6126, KL-Loss   58.0663, KL-Weight  0.016
TRAIN Batch 0900/1314, Loss  104.5415, NLL-Loss  103.5031, KL-Loss   57.7367, KL-Weight  0.018
TRAIN Batch 0950/1314, Loss   96.0210, NLL-Loss   94.9319, KL-Loss   53.5646, KL-Weight  0.020
TRAIN Batch 1000/1314, Loss  102.4422, NLL-Loss  101.1491, KL-Loss   56.2795, KL-Weight  0.023
TRAIN Batch 1050/1314, Loss  118.6219, NLL-Loss  117.2702, KL-Loss   52.0748, KL-Weight  0.026
TRAIN Batch 1100/1314, Loss  130.7773, NLL-Loss  129.2106, KL-Loss   53.4471, KL-Weight  0.029
TRAIN Batch 1150/1314, Loss  109.3414, NLL-Loss  107.7636, KL-Loss   47.6872, KL-Weight  0.033
TRAIN Batch 1200/1314, Loss  114.7617, NLL-Loss  112.9445, KL-Loss   48.6833, KL-Weight  0.037
TRAIN Batch 1250/1314, Loss  111.6586, NLL-Loss  109.6696, KL-Loss   47.2576, KL-Weight  0.042
TRAIN Batch 1300/1314, Loss   90.2617, NLL-Loss   88.2501, KL-Loss   42.4157, KL-Weight  0.047
TRAIN Batch 1314/1314, Loss   82.4088, NLL-Loss   80.1875, KL-Loss   45.3043, KL-Weight  0.049
TRAIN Epoch 00/1, Mean ELBO  119.8328
Model saved at bin/2025-Dec-16-16:16:33/E0.pytorch
VALID Batch 0000/105, Loss  126.4861, NLL-Loss  124.4176, KL-Loss   42.0854, KL-Weight  0.049
VALID Batch 0050/105, Loss  123.9015, NLL-Loss  121.5696, KL-Loss   47.4452, KL-Weight  0.049
VALID Batch 0100/105, Loss   89.4296, NLL-Loss   87.3504, KL-Loss   42.3028, KL-Weight  0.049
VALID Batch 0105/105, Loss   91.1230, NLL-Loss   88.8317, KL-Loss   46.6201, KL-Weight  0.049
VALID Epoch 00/1, Mean ELBO  105.0135
