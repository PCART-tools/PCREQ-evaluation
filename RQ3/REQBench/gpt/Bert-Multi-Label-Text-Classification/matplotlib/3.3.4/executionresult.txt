12/16/2025 23:27:29 - INFO - root -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, arch='bert', data_name='kaggle', do_data=False, do_lower_case=True, do_test=False, do_train=True, epochs=1, eval_batch_size=8, eval_max_seq_len=256, fp16=False, fp16_opt_level='O1', grad_clip=1.0, gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, loss_scale=0, mode='min', monitor='valid_loss', n_gpu='2', predict_checkpoints=0, resume_path='', save_best=True, seed=42, sorted=1, train_batch_size=8, train_max_seq_len=256, valid_size=0.2, warmup_proportion=0.1, weight_decay=0.01)
12/16/2025 23:27:30 - INFO - root -   Loading examples from cached file pybert/dataset/cached_train_examples_bert
12/16/2025 23:27:30 - INFO - root -   Loading features from cached file pybert/dataset/cached_train_features_256_bert
12/16/2025 23:27:35 - INFO - root -   sorted data by th length of input
12/16/2025 23:27:38 - INFO - root -   Loading examples from cached file pybert/dataset/cached_valid_examples_bert
12/16/2025 23:27:38 - INFO - root -   Loading features from cached file pybert/dataset/cached_valid_features_256_bert
12/16/2025 23:27:40 - INFO - root -   initializing model
12/16/2025 23:27:40 - INFO - transformers.configuration_utils -   loading configuration file pybert/pretrain/bert/base-uncased/config.json
12/16/2025 23:27:40 - INFO - transformers.configuration_utils -   Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 6,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522
}

12/16/2025 23:27:40 - INFO - transformers.modeling_utils -   loading weights file pybert/pretrain/bert/base-uncased/pytorch_model.bin
12/16/2025 23:27:43 - INFO - transformers.modeling_utils -   Weights of BertForMultiLable not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
12/16/2025 23:27:43 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMultiLable: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
12/16/2025 23:27:43 - INFO - root -   initializing callbacks
12/16/2025 23:27:43 - INFO - root -   ***** Running training *****
12/16/2025 23:27:43 - INFO - root -     Num examples = 127657
12/16/2025 23:27:43 - INFO - root -     Num Epochs = 1
12/16/2025 23:27:43 - INFO - root -     Total train batch size (w. parallel, distributed & accumulation) = 8
12/16/2025 23:27:43 - INFO - root -     Gradient Accumulation steps = 1
12/16/2025 23:27:43 - INFO - root -     Total optimization steps = 15958
12/16/2025 23:27:46 - INFO - root -   Epoch 1/1
[Training] 1/15958 [..............................] - ETA: 1:02:57  accuracy: 0.4792 - loss: 0.6911 [Training] 2/15958 [..............................] - ETA: 57:47  accuracy: 0.5208 - loss: 0.6711 [Training] 3/15958 [..............................] - ETA: 55:33  accuracy: 0.5000 - loss: 0.6837 [Training] 4/15958 [..............................] - ETA: 54:40  accuracy: 0.5417 - loss: 0.6785 [Training] 5/15958 [..............................] - ETA: 54:05  accuracy: 0.5625 - loss: 0.6802 [Training] 6/15958 [..............................] - ETA: 53:38  accuracy: 0.4583 - loss: 0.6919 [Training] 7/15958 [..............................] - ETA: 53:27  accuracy: 0.6458 - loss: 0.6642 [Training] 8/15958 [..............................] - ETA: 53:21  accuracy: 0.5417 - loss: 0.6754 [Training] 9/15958 [..............................] - ETA: 53:13  accuracy: 0.6250 - loss: 0.6703 [Training] 10/15958 [..............................] - ETA: 53:02  accuracy: 0.5833 - loss: 0.6792 [Training] 11/15958 [..............................] - ETA: 52:51  accuracy: 0.5000 - loss: 0.6780 [Training] 12/15958 [..............................] - ETA: 52:47  accuracy: 0.5625 - loss: 0.6769 [Training] 13/15958 [..............................] - ETA: 52:37  accuracy: 0.5625 - loss: 0.6664 [Training] 14/15958 [..............................] - ETA: 52:34  accuracy: 0.5625 - loss: 0.6850 [Training] 15/15958 [..............................] - ETA: 52:24  accuracy: 0.4583 - loss: 0.7028 [Training] 16/15958 [..............................] - ETA: 52:19  accuracy: 0.5000 - loss: 0.6852 [Training] 17/15958 [..............................] - ETA: 52:18  accuracy: 0.5625 - loss: 0.6733 [Training] 18/15958 [..............................] - ETA: 52:13  accuracy: 0.5625 - loss: 0.6573 [Training] 19/15958 [..............................] - ETA: 52:11  accuracy: 0.5625 - loss: 0.6700 [Training] 20/15958 [..............................] - ETA: 52:09  accuracy: 0.5208 - loss: 0.6987 [Training] 21/15958 [..............................] - ETA: 52:08  accuracy: 0.5833 - loss: 0.6676 [Training] 22/15958 [..............................] - ETA: 52:01  accuracy: 0.6042 - loss: 0.6652 [Training] 23/15958 [..............................] - ETA: 52:00  accuracy: 0.4792 - loss: 0.6771 [Training] 24/15958 [..............................] - ETA: 52:00  accuracy: 0.5625 - loss: 0.6679 [Training] 25/15958 [..............................] - ETA: 52:00  accuracy: 0.6250 - loss: 0.6721 [Training] 26/15958 [..............................] - ETA: 51:59  accuracy: 0.6250 - loss: 0.6622 [Training] 27/15958 [..............................] - ETA: 51:59  accuracy: 0.6042 - loss: 0.6602 [Training] 28/15958 [..............................] - ETA: 51:58  accuracy: 0.5625 - loss: 0.6911 [Training] 29/15958 [..............................] - ETA: 51:57  accuracy: 0.5417 - loss: 0.6877 [Training] 30/15958 [..............................] - ETA: 51:57  accuracy: 0.5417 - loss: 0.6756 [Training] 31/15958 [..............................] - ETA: 51:55  accuracy: 0.5208 - loss: 0.6843 [Training] 32/15958 [..............................] - ETA: 51:55  accuracy: 0.5625 - loss: 0.6617 [Training] 33/15958 [..............................] - ETA: 51:53  accuracy: 0.5625 - loss: 0.6713 [Training] 34/15958 [..............................] - ETA: 51:52  accuracy: 0.5625 - loss: 0.6799 [Training] 35/15958 [..............................] - ETA: 51:49  accuracy: 0.5625 - loss: 0.6484 [Training] 36/15958 [..............................] - ETA: 51:48  accuracy: 0.6250 - loss: 0.6594 [Training] 37/15958 [..............................] - ETA: 51:48  accuracy: 0.6250 - loss: 0.6472 [Training] 38/15958 [..............................] - ETA: 51:46  accuracy: 0.5833 - loss: 0.6570 [Training] 39/15958 [..............................] - ETA: 51:46  accuracy: 0.6458 - loss: 0.6484 [Training] 40/15958 [..............................] - ETA: 51:45  accuracy: 0.5625 - loss: 0.6545 [Training] 41/15958 [..............................] - ETA: 51:46  accuracy: 0.5833 - loss: 0.6604 [Training] 42/15958 [..............................] - ETA: 51:46  accuracy: 0.7083 - loss: 0.6360 [Training] 43/15958 [..............................] - ETA: 51:48  accuracy: 0.5833 - loss: 0.6707 [Training] 44/15958 [..............................] - ETA: 51:48  accuracy: 0.5625 - loss: 0.6479 [Training] 45/15958 [..............................] - ETA: 51:49  accuracy: 0.6458 - loss: 0.6502 [Training] 46/15958 [..............................] - ETA: 51:49  accuracy: 0.6458 - loss: 0.6539 [Training] 47/15958 [..............................] - ETA: 51:49  accuracy: 0.6250 - loss: 0.6496 [Training] 48/15958 [..............................] - ETA: 51:50  accuracy: 0.6250 - loss: 0.6484 [Training] 49/15958 [..............................] - ETA: 51:50  accuracy: 0.5208 - loss: 0.6687 [Training] 50/15958 [..............................] - ETA: 51:50  accuracy: 0.5208 - loss: 0.6698 [Training] 51/15958 [..............................] - ETA: 51:50  accuracy: 0.6667 - loss: 0.6372 [Training] 52/15958 [..............................] - ETA: 51:49  accuracy: 0.6667 - loss: 0.6367 [Training] 53/15958 [..............................] - ETA: 51:49  accuracy: 0.5417 - loss: 0.6551 [Training] 54/15958 [..............................] - ETA: 51:50  accuracy: 0.5625 - loss: 0.6622 [Training] 55/15958 [..............................] - ETA: 51:49  accuracy: 0.5625 - loss: 0.6463 [Training] 56/15958 [..............................] - ETA: 51:47  accuracy: 0.6042 - loss: 0.6513 [Training] 57/15958 [..............................] - ETA: 51:47  accuracy: 0.6458 - loss: 0.6366 [Training] 58/15958 [..............................] - ETA: 51:47  accuracy: 0.6250 - loss: 0.6484 [Training] 59/15958 [..............................] - ETA: 51:46  accuracy: 0.7292 - loss: 0.6172 [Training] 60/15958 [..............................] - ETA: 51:44  accuracy: 0.6667 - loss: 0.6310 [Training] 61/15958 [..............................] - ETA: 51:44  accuracy: 0.6875 - loss: 0.6584 [Training] 62/15958 [..............................] - ETA: 51:44  accuracy: 0.7083 - loss: 0.6278 [Training] 63/15958 [..............................] - ETA: 51:44  accuracy: 0.7500 - loss: 0.6132 [Training] 64/15958 [..............................] - ETA: 51:44  accuracy: 0.6667 - loss: 0.6205 [Training] 65/15958 [..............................] - ETA: 51:45  accuracy: 0.6250 - loss: 0.6262 [Training] 66/15958 [..............................] - ETA: 51:44  accuracy: 0.7708 - loss: 0.6214 [Training] 67/15958 [..............................] - ETA: 51:44  accuracy: 0.7292 - loss: 0.6146 [Training] 68/15958 [..............................] - ETA: 51:45  accuracy: 0.6667 - loss: 0.6187 [Training] 69/15958 [..............................] - ETA: 51:45  accuracy: 0.6875 - loss: 0.6320 [Training] 70/15958 [..............................] - ETA: 51:45  accuracy: 0.6250 - loss: 0.6197 [Training] 71/15958 [..............................] - ETA: 51:44  accuracy: 0.7500 - loss: 0.6162 [Training] 72/15958 [..............................] - ETA: 51:45  accuracy: 0.6458 - loss: 0.6396 [Training] 73/15958 [..............................] - ETA: 51:45  accuracy: 0.7083 - loss: 0.6337 [Training] 74/15958 [..............................] - ETA: 51:45  accuracy: 0.6875 - loss: 0.6200 [Training] 75/15958 [..............................] - ETA: 51:44  accuracy: 0.7500 - loss: 0.5951 [Training] 76/15958 [..............................] - ETA: 51:43  accuracy: 0.7083 - loss: 0.6077 [Training] 77/15958 [..............................] - ETA: 51:43  accuracy: 0.7708 - loss: 0.5908 [Training] 78/15958 [..............................] - ETA: 51:43  accuracy: 0.6875 - loss: 0.6134 [Training] 79/15958 [..............................] - ETA: 51:44  accuracy: 0.7292 - loss: 0.5913 [Training] 80/15958 [..............................] - ETA: 51:43  accuracy: 0.8125 - loss: 0.5849 [Training] 81/15958 [..............................] - ETA: 51:43  accuracy: 0.6667 - loss: 0.6115 [Training] 82/15958 [..............................] - ETA: 51:43  accuracy: 0.7292 - loss: 0.6098 [Training] 83/15958 [..............................] - ETA: 51:43  accuracy: 0.6667 - loss: 0.6158 [Training] 84/15958 [..............................] - ETA: 51:42  accuracy: 0.6875 - loss: 0.5898 [Training] 85/15958 [..............................] - ETA: 51:43  accuracy: 0.6875 - loss: 0.6112 [Training] 86/15958 [..............................] - ETA: 51:43  accuracy: 0.7292 - loss: 0.5958 [Training] 87/15958 [..............................] - ETA: 51:42  accuracy: 0.6667 - loss: 0.6120 [Training] 88/15958 [..............................] - ETA: 51:43  accuracy: 0.8333 - loss: 0.5747 [Training] 89/15958 [..............................] - ETA: 51:43  accuracy: 0.7500 - loss: 0.5708 [Training] 90/15958 [..............................] - ETA: 51:43  accuracy: 0.8333 - loss: 0.6008 [Training] 91/15958 [..............................] - ETA: 51:43  accuracy: 0.7917 - loss: 0.5699 [Training] 92/15958 [..............................] - ETA: 51:44  accuracy: 0.8542 - loss: 0.5588 [Training] 93/15958 [..............................] - ETA: 51:44  accuracy: 0.8125 - loss: 0.5617 [Training] 94/15958 [..............................] - ETA: 51:44  accuracy: 0.7917 - loss: 0.5523 [Training] 95/15958 [..............................] - ETA: 51:44  accuracy: 0.7708 - loss: 0.5863 [Training] 96/15958 [..............................] - ETA: 51:45  accuracy: 0.7292 - loss: 0.5720 [Training] 97/15958 [..............................] - ETA: 51:45  accuracy: 0.6875 - loss: 0.6061 [Training] 98/15958 [..............................] - ETA: 51:45  accuracy: 0.8333 - loss: 0.5656 [Training] 99/15958 [..............................] - ETA: 51:43  accuracy: 0.8125 - loss: 0.5645 [Training] 100/15958 [..............................] - ETA: 51:44  accuracy: 0.7083 - loss: 0.5895 [Training] 101/15958 [..............................] - ETA: 51:44  accuracy: 0.8958 - loss: 0.5622 
------------- train result --------------
label:toxic - auc: 0.5396
label:severe_toxic - auc: 0.4532
label:obscene - auc: 0.4662
label:threat - auc: 0.4325
label:insult - auc: 0.5454
label:identity_hate - auc: 0.6358
[Evaluating] 1/3990 [..............................] - ETA: 3:51[Evaluating] 2/3990 [..............................] - ETA: 3:45[Evaluating] 3/3990 [..............................] - ETA: 3:07[Evaluating] 4/3990 [..............................] - ETA: 3:15[Evaluating] 5/3990 [..............................] - ETA: 3:20[Evaluating] 6/3990 [..............................] - ETA: 3:26[Evaluating] 7/3990 [..............................] - ETA: 3:29[Evaluating] 8/3990 [..............................] - ETA: 3:28[Evaluating] 9/3990 [..............................] - ETA: 3:24[Evaluating] 10/3990 [..............................] - ETA: 3:26[Evaluating] 11/3990 [..............................] - ETA: 3:27[Evaluating] 12/3990 [..............................] - ETA: 3:28[Evaluating] 13/3990 [..............................] - ETA: 3:21[Evaluating] 14/3990 [..............................] - ETA: 3:22[Evaluating] 15/3990 [..............................] - ETA: 3:18[Evaluating] 16/3990 [..............................] - ETA: 3:11[Evaluating] 17/3990 [..............................] - ETA: 3:13[Evaluating] 18/3990 [..............................] - ETA: 3:08[Evaluating] 19/3990 [..............................] - ETA: 3:10[Evaluating] 20/3990 [..............................] - ETA: 3:11[Evaluating] 21/3990 [..............................] - ETA: 3:12[Evaluating] 22/3990 [..............................] - ETA: 3:13[Evaluating] 23/3990 [..............................] - ETA: 3:09[Evaluating] 24/3990 [..............................] - ETA: 3:10[Evaluating] 25/3990 [..............................] - ETA: 3:07[Evaluating] 26/3990 [..............................] - ETA: 3:07[Evaluating] 27/3990 [..............................] - ETA: 3:08[Evaluating] 28/3990 [..............................] - ETA: 3:05[Evaluating] 29/3990 [..............................] - ETA: 3:07[Evaluating] 30/3990 [..............................] - ETA: 3:05[Evaluating] 31/3990 [..............................] - ETA: 3:03[Evaluating] 32/3990 [..............................] - ETA: 3:00[Evaluating] 33/3990 [..............................] - ETA: 3:01[Evaluating] 34/3990 [..............................] - ETA: 2:59[Evaluating] 35/3990 [..............................] - ETA: 3:00[Evaluating] 36/3990 [..............................] - ETA: 2:58[Evaluating] 37/3990 [..............................] - ETA: 2:56[Evaluating] 38/3990 [..............................] - ETA: 2:57[Evaluating] 39/3990 [..............................] - ETA: 2:58[Evaluating] 40/3990 [..............................] - ETA: 2:56[Evaluating] 41/3990 [..............................] - ETA: 2:54[Evaluating] 42/3990 [..............................] - ETA: 2:54[Evaluating] 43/3990 [..............................] - ETA: 2:52[Evaluating] 44/3990 [..............................] - ETA: 2:53[Evaluating] 45/3990 [..............................] - ETA: 2:54[Evaluating] 46/3990 [..............................] - ETA: 2:53[Evaluating] 47/3990 [..............................] - ETA: 2:52[Evaluating] 48/3990 [..............................] - ETA: 2:51[Evaluating] 49/3990 [..............................] - ETA: 2:49[Evaluating] 50/3990 [..............................] - ETA: 2:50[Evaluating] 51/3990 [..............................] - ETA: 2:49[Evaluating] 52/3990 [..............................] - ETA: 2:48[Evaluating] 53/3990 [..............................] - ETA: 2:49[Evaluating] 54/3990 [..............................] - ETA: 2:50[Evaluating] 55/3990 [..............................] - ETA: 2:51[Evaluating] 56/3990 [..............................] - ETA: 2:50[Evaluating] 57/3990 [..............................] - ETA: 2:51[Evaluating] 58/3990 [..............................] - ETA: 2:51[Evaluating] 59/3990 [..............................] - ETA: 2:51[Evaluating] 60/3990 [..............................] - ETA: 2:52[Evaluating] 61/3990 [..............................] - ETA: 2:51[Evaluating] 62/3990 [..............................] - ETA: 2:52[Evaluating] 63/3990 [..............................] - ETA: 2:51[Evaluating] 64/3990 [..............................] - ETA: 2:51[Evaluating] 65/3990 [..............................] - ETA: 2:50[Evaluating] 66/3990 [..............................] - ETA: 2:51[Evaluating] 67/3990 [..............................] - ETA: 2:50[Evaluating] 68/3990 [..............................] - ETA: 2:51[Evaluating] 69/3990 [..............................] - ETA: 2:52[Evaluating] 70/3990 [..............................] - ETA: 2:52[Evaluating] 71/3990 [..............................] - ETA: 2:52[Evaluating] 72/3990 [..............................] - ETA: 2:53[Evaluating] 73/3990 [..............................] - ETA: 2:54[Evaluating] 74/3990 [..............................] - ETA: 2:54[Evaluating] 75/3990 [..............................] - ETA: 2:55[Evaluating] 76/3990 [..............................] - ETA: 2:55[Evaluating] 77/3990 [..............................] - ETA: 2:55[Evaluating] 78/3990 [..............................] - ETA: 2:56[Evaluating] 79/3990 [..............................] - ETA: 2:56[Evaluating] 80/3990 [..............................] - ETA: 2:56[Evaluating] 81/3990 [..............................] - ETA: 2:56[Evaluating] 82/3990 [..............................] - ETA: 2:55[Evaluating] 83/3990 [..............................] - ETA: 2:56[Evaluating] 84/3990 [..............................] - ETA: 2:55[Evaluating] 85/3990 [..............................] - ETA: 2:55[Evaluating] 86/3990 [..............................] - ETA: 2:55[Evaluating] 87/3990 [..............................] - ETA: 2:55[Evaluating] 88/3990 [..............................] - ETA: 2:56[Evaluating] 89/3990 [..............................] - ETA: 2:55[Evaluating] 90/3990 [..............................] - ETA: 2:54[Evaluating] 91/3990 [..............................] - ETA: 2:54[Evaluating] 92/3990 [..............................] - ETA: 2:53[Evaluating] 93/3990 [..............................] - ETA: 2:5412/16/2025 23:28:13 - INFO - root -   
Epoch: 1 -  loss: 0.6388 - auc: 0.5352 - valid_loss: 0.6054 - valid_auc: 0.5604 
12/16/2025 23:28:13 - INFO - root -   
Epoch 1: valid_loss improved from inf to 0.60544
12/16/2025 23:28:13 - INFO - transformers.configuration_utils -   Configuration saved in pybert/output/checkpoints/bert/config.json
12/16/2025 23:28:15 - INFO - transformers.modeling_utils -   Model weights saved in pybert/output/checkpoints/bert/pytorch_model.bin
[Evaluating] 94/3990 [..............................] - ETA: 2:54[Evaluating] 95/3990 [..............................] - ETA: 2:53[Evaluating] 96/3990 [..............................] - ETA: 2:54[Evaluating] 97/3990 [..............................] - ETA: 2:54[Evaluating] 98/3990 [..............................] - ETA: 2:53[Evaluating] 99/3990 [..............................] - ETA: 2:54[Evaluating] 100/3990 [..............................] - ETA: 2:54[Evaluating] 101/3990 [..............................] - ETA: 2:54------------- valid result --------------
label:toxic - auc: 0.5082
label:severe_toxic - auc: 0.4845
label:obscene - auc: 0.6338
label:threat - auc: 0.6818
label:insult - auc: 0.5749
label:identity_hate - auc: 0.6106
